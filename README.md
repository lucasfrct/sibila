# Sibila - Sistema RAG para An√°lise de Documentos Jur√≠dicos

As sibilas eram figuras da mitologia inspiradas pelos deuses para profetizar. Esse reposit√≥rio implementa um sistema avan√ßado de **RAG (Retrieval Augmented Generation)** especializado em an√°lise de documentos jur√≠dicos, capaz de responder perguntas baseadas em documentos fornecidos pelo usu√°rio atrav√©s de intelig√™ncia artificial.

## üéØ Vis√£o Geral do Sistema

Sibila √© uma aplica√ß√£o Python que combina processamento de linguagem natural, bancos de dados vetoriais e modelos de linguagem para criar um assistente inteligente para an√°lise documental. O sistema processa documentos PDF (especialmente legisla√ß√£o), extrai conhecimento estruturado e fornece respostas precisas atrav√©s de uma API REST.

## üöÄ Como Executar Localmente

### Configura√ß√£o R√°pida

1. **Clone o reposit√≥rio**
```bash
git clone https://github.com/lucasfrct/sibila.git
cd sibila
```

2. **Configure o ambiente**
```bash
cp .env.example .env
```

3. **Instale as depend√™ncias**
```bash
pip install -r requirements.txt
```

4. **Instale e configure o Ollama**
```bash
# Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Iniciar o servidor Ollama
ollama serve

# Em outro terminal, baixar o modelo
ollama pull llama3
```

5. **Execute a aplica√ß√£o**
```bash
python main.py
```

A aplica√ß√£o estar√° dispon√≠vel em `http://localhost:3000`

### Execu√ß√£o sem Hot Reload
```bash
python main.py --no-reload
```

### üìñ Guia Completo
Para instru√ß√µes detalhadas, consulte: **[SETUP_LOCAL.md](./SETUP_LOCAL.md)**

## üèóÔ∏è Arquitetura do Sistema

### Pipeline de Processamento de Documentos

O sistema implementa um pipeline sofisticado para processamento de documentos PDF:

#### 1. **M√≥dulo de Leitura (`src/modules/document/reader.py`)**
- **Biblioteca principal**: `pdfplumber` para extra√ß√£o precisa de texto
- **Funcionalidades**:
  - Leitura completa de documentos PDF
  - Extra√ß√£o de p√°ginas espec√≠ficas com controle de intervalo
  - Mec√¢nica de limite de p√°ginas com valida√ß√£o autom√°tica
  - Tratamento de erros e logging detalhado

#### 2. **Sistema de Metadados (`src/modules/document/`)**
- **Metadados de p√°gina** (`page_metadata.py`): Informa√ß√µes estruturais de cada p√°gina
- **Metadados de par√°grafo** (`paragraph_metadata.py`): Segmenta√ß√£o em par√°grafos com posicionamento
- **Metadados de frase** (`phrase_metadata.py`): Granularidade no n√≠vel de senten√ßa
- **Reposit√≥rios de dados**: Persist√™ncia SQLite com padr√£o Repository

#### 3. **An√°lise de Legisla√ß√£o (`src/modules/analysis/legislation.py`)**
- **Divis√£o autom√°tica em artigos**: Regex avan√ßado para identificar estrutura legal
- **Classifica√ß√£o de categorias jur√≠dicas**:
  - Direito Constitucional, Trabalhista, Civil, Penal
  - Direito Administrativo, Tribut√°rio, Ambiental, etc.
- **Identifica√ß√£o de tipos normativos**: Lei, Decreto, Portaria, Resolu√ß√£o, etc.
- **Sistema de efic√°cia normativa**: Plena, Limitada, Contida

### Sistema de Processamento de Linguagem Natural

#### 1. **M√≥dulo NLP (`src/modules/nlp/`)**

**Bag of Words (BoW)** (`bow.py`):
```python
# Gera√ß√£o de vocabul√°rio com sklearn CountVectorizer
def generate_bow(content: str) -> dict:
    processed_content = Str.removal_stopwords(content)
    vectorizer = CountVectorizer()
    bow = vectorizer.fit_transform([processed_content])
    # Retorna dicion√°rio palavra: frequ√™ncia
```

**Classifica√ß√£o de Texto** (`classifier.py`):
- **Algoritmo**: Regress√£o Log√≠stica com TF-IDF
- **Pipeline**: Vetoriza√ß√£o ‚Üí Treinamento ‚Üí Predi√ß√£o
- **Persist√™ncia**: Modelos salvos em formato joblib
- **Avalia√ß√£o**: Classification report do scikit-learn

**An√°lise de Sentimento** (`sentiment.py`):
- Classifica√ß√£o emocional de textos jur√≠dicos
- Integra√ß√£o com pipeline de an√°lise de documentos

**Extra√ß√£o de Features** (`featureextractor.py`):
- Caracter√≠sticas lingu√≠sticas relevantes para classifica√ß√£o
- Otimiza√ß√£o para dom√≠nio jur√≠dico

#### 2. **Pr√©-processamento Avan√ßado**
- **Remo√ß√£o de stop words**: Lista customizada para portugu√™s jur√≠dico
- **Normaliza√ß√£o de texto**: Tratamento de caracteres especiais
- **Tokeniza√ß√£o inteligente**: Preserva√ß√£o de termos jur√≠dicos espec√≠ficos

### Banco de Dados Vetorial e Recupera√ß√£o

#### ChromaDB Integration (`src/modules/database/chromadbvector.py`)

**Configura√ß√£o do Cliente**:
```python
def client(path: str = "./data/.chromadb") -> chromadb.ClientAPI:
    return chromadb.PersistentClient(path=path)
```

**Funcionalidades principais**:
- **Armazenamento vetorial**: Embeddings de documentos e chunks
- **Busca por similaridade**: Recupera√ß√£o baseada em dist√¢ncia cossenoidal
- **Gest√£o de cole√ß√µes**: Organiza√ß√£o hier√°rquica de documentos
- **Detec√ß√£o de conflitos**: Preven√ß√£o de duplica√ß√£o de IDs
- **Indexa√ß√£o HNSW**: Performance otimizada para buscas aproximadas

### Integra√ß√£o com Modelos de Linguagem

#### 1. **Suporte Dual para LLMs (`src/models/`)**

**Ollama Local** (`ollama.py`):
```python
class ModelOllama:
    def __init__(self, model: str = "sibila"):
        self.client = ollama
        self.temperature = 0.5           # Controle de aleatoriedade
        self.max_tokens = 4096          # Limite de tokens de sa√≠da
        self.context = 2048             # Janela de contexto
        self.diffusion_of_hallucination = 0.8  # Anti-alucina√ß√£o
```

**OpenAI Cloud** (`open_ai.py`):
- Integra√ß√£o com API oficial da OpenAI
- Fallback para quando Ollama n√£o est√° dispon√≠vel
- Suporte a modelos GPT-3.5/GPT-4

#### 2. **Sistema de Prompts Avan√ßado (`src/modules/prompts/prompts.py`)**

**Persona de Bibliotec√°rio IA**:
```python
def resume(documents: str) -> str:
    prompt_template = """Voc√™ √© um assistente de IA com experi√™ncia de um bibliotec√°rio organizado.
    Voc√™ tem uma alta capacidade de organizar ideias, documentos, assuntos e sess√µes.
    Os documentos abaixo apresentam as fontes atualizadas e devem ser consideradas como verdade.
    Suas respostas s√£o apenas baseadas no documento e n√£o deve inventar, adicionar ou alucinar.
    Gere um resumo de no m√°ximo 50 palavras...
    """
```

**Tipos de prompts implementados**:
- **Keywords**: Extra√ß√£o de palavras-chave relevantes
- **Resume**: Resumos concisos de documentos
- **Names**: Identifica√ß√£o de pessoas e autores citados
- **Publisher**: Identifica√ß√£o de editoras e fontes

### API REST e Endpoints

#### Estrutura da API (`src/routes/routes.py`)

**Endpoints principais**:
- `GET /api/v1/health` - Status do sistema
- `GET /api/v1/dataset` - Listagem de datasets dispon√≠veis
- `GET /api/v1/corpus` - Listagem do corpus gerado
- `POST /api/v1/corpus/generate` - Gera√ß√£o ass√≠ncrona de corpus

**Endpoints planejados** (comentados):
- `/api/v1/catalog/search` - Busca no cat√°logo
- `/api/v1/catalog/indexer` - Indexa√ß√£o de documentos
- `/api/v1/completions` - Chat completions
- `/api/v1/completions/history` - Hist√≥rico de conversas

### Gera√ß√£o e An√°lise de Corpus

#### Sistema de Corpus (`src/modules/corpus/corpus.py`)

**Processamento multithread**:
```python
def doc_with_articles(path: str, page_init: int = 1, page_final: int = -1):
    doc_info = DocService.info(path)
    doc_file = DocService.pdf_content(doc['path'], page_init, page_final)
    doc['articles'] = Legislation.split_into_articles(doc_file)
    return doc
```

**Features do sistema de corpus**:
- **Extra√ß√£o autom√°tica de artigos**: Regex pattern matching para estrutura legal
- **Anota√ß√£o inteligente**: LLM auxilia na categoriza√ß√£o e resumo
- **Processamento ass√≠ncrono**: ThreadPoolExecutor para performance
- **Controle de progresso**: Shared memory counters para tracking
- **Exporta√ß√£o CSV**: Formato estruturado para an√°lise posterior

### Sistema de Cat√°loga√ß√£o

#### M√≥dulo de Cat√°logo (`src/modules/catalog/`)

**Classe Catalog** (`catalog.py`):
```python
@dataclass
class Catalog:
    id: int = 0
    path: str = ""
    name: str = ""
    size: int = 0
    pages: int = 1
    mimetype: str = ""
    title: str = ""
    resume: str = ""
    categories: str = ""
```

**Funcionalidades**:
- **Indexa√ß√£o autom√°tica**: Metadados extra√≠dos automaticamente
- **Categoriza√ß√£o**: Classifica√ß√£o por √°rea jur√≠dica
- **Busca sem√¢ntica**: Integra√ß√£o com ChromaDB
- **Reposit√≥rio de dados**: Persist√™ncia e recupera√ß√£o otimizada

## üìä Stack Tecnol√≥gico Detalhado

### Core Framework
- **Flask 3.0.3**: Framework web Python com suporte ass√≠ncrono
- **Python 3.9+**: Linguagem base com type hints

### Processamento de Documentos
- **pdfplumber 0.11.1**: Extra√ß√£o precisa de texto de PDFs
- **PDFMiner.six**: Parser avan√ßado de estrutura PDF
- **PyPDF2 3.0.1**: Manipula√ß√£o de metadados PDF

### Machine Learning & NLP
- **scikit-learn 1.5.0**: Classifica√ß√£o e vetoriza√ß√£o
- **spaCy 3.7.5**: Processamento de linguagem natural
- **NLTK 3.8.1**: Toolkit de processamento lingu√≠stico
- **transformers** (via Ollama): Modelos transformer

### Banco de Dados
- **ChromaDB 0.5.3**: Banco vetorial com HNSW indexing
- **SQLite**: Banco relacional para metadados
- **chroma-hnswlib 0.7.3**: Otimiza√ß√£o de indexa√ß√£o

### Integra√ß√£o LLM
- **ollama 0.3.3**: Cliente para modelos locais
- **openai 1.10.0**: Cliente oficial OpenAI
- **httpx 0.27.0**: Cliente HTTP ass√≠ncrono

### Utilit√°rios e Suporte
- **asyncio**: Programa√ß√£o ass√≠ncrona nativa
- **joblib 1.4.2**: Serializa√ß√£o de modelos ML
- **tqdm 4.66.2**: Barras de progresso
- **python-dotenv 1.0.1**: Configura√ß√£o via vari√°veis de ambiente

## üóÇÔ∏è Estrutura de Diret√≥rios

```
sibila/
‚îú‚îÄ‚îÄ src/                    # C√≥digo fonte principal
‚îÇ   ‚îú‚îÄ‚îÄ config/            # Configura√ß√µes (Ollama, OpenAI)
‚îÇ   ‚îú‚îÄ‚îÄ models/            # Integra√ß√µes com LLMs
‚îÇ   ‚îú‚îÄ‚îÄ modules/           # M√≥dulos funcionais
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analysis/      # An√°lise jur√≠dica
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ catalog/       # Sistema de cataloga√ß√£o
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ corpus/        # Gera√ß√£o de corpus
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database/      # Bancos de dados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ document/      # Processamento de documentos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nlp/          # Processamento de linguagem
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompts/      # Sistema de prompts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ response/     # Formata√ß√£o de respostas
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ viz/          # Visualiza√ß√µes
‚îÇ   ‚îú‚îÄ‚îÄ routes/           # Endpoints da API
‚îÇ   ‚îú‚îÄ‚îÄ routines/         # Rotinas de migra√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ utils/            # Utilit√°rios diversos
‚îÇ   ‚îî‚îÄ‚îÄ server.py         # Configura√ß√£o Flask
‚îú‚îÄ‚îÄ dataset/              # Dados de treinamento
‚îÇ   ‚îú‚îÄ‚îÄ sources/         # Documentos fonte (PDFs)
‚îÇ   ‚îú‚îÄ‚îÄ corpus/          # Corpus processado (CSV)
‚îÇ   ‚îî‚îÄ‚îÄ _library/        # Biblioteca de refer√™ncia
‚îú‚îÄ‚îÄ data/                # Persist√™ncia de dados
‚îÇ   ‚îî‚îÄ‚îÄ .chromadb/       # Banco vetorial ChromaDB
‚îú‚îÄ‚îÄ tests/               # Testes automatizados
‚îî‚îÄ‚îÄ main.py             # Ponto de entrada da aplica√ß√£o
```

## üîÑ Fluxo de Dados do Sistema

### 1. **Ingest√£o de Documentos**
```
PDF Input ‚Üí pdfplumber ‚Üí Text Extraction ‚Üí Metadata Generation ‚Üí SQLite Storage
```

### 2. **Processamento de Corpus**
```
Legal Documents ‚Üí Article Splitting ‚Üí LLM Annotation ‚Üí ChromaDB Indexing ‚Üí CSV Export
```

### 3. **Pipeline RAG**
```
User Query ‚Üí Embedding Generation ‚Üí Vector Search ‚Üí Context Retrieval ‚Üí LLM Completion ‚Üí Structured Response
```

### 4. **An√°lise Jur√≠dica**
```
Legal Text ‚Üí Category Classification ‚Üí Normative Type ‚Üí Efficacy Analysis ‚Üí Structured Metadata
```

## ‚öôÔ∏è Configura√ß√£o Avan√ßada

### Vari√°veis de Ambiente (.env)
```bash
OPENAI_API_KEY=        # Chave API OpenAI (opcional)
OLLAMA_PROXY_URL=      # URL proxy Ollama (opcional)
```

### Par√¢metros do Modelo Ollama
```python
temperature = 0.5                    # Controle de aleatoriedade (0.0-1.0)
max_tokens = 4096                   # Limite de tokens de sa√≠da
context = 2048                      # Janela de contexto
diffusion_of_hallucination = 0.8   # Anti-alucina√ß√£o (0.0-1.0)
diversification_rate = 0.0         # Taxa de diversifica√ß√£o
penalty_rate = 1.1                 # Penaliza√ß√£o de repeti√ß√£o
```

## üéØ Casos de Uso Espec√≠ficos

### 1. **An√°lise de Legisla√ß√£o**
- Processamento autom√°tico da Constitui√ß√£o Federal
- Extra√ß√£o de artigos e incisos
- Categoriza√ß√£o por √°rea jur√≠dica
- Gera√ß√£o de resumos e palavras-chave

### 2. **Busca Sem√¢ntica em Documentos**
- Consultas em linguagem natural
- Recupera√ß√£o baseada em similaridade
- Cita√ß√£o autom√°tica de fontes
- Ranking de relev√¢ncia

### 3. **Assistente Jur√≠dico IA**
- Respostas baseadas apenas em documentos
- Preven√ß√£o de alucina√ß√µes
- Formato de resposta estruturado
- Rastreabilidade de fontes

## üöÄ Roadmap e Funcionalidades Planejadas

### Implementadas ‚úÖ
- [x] Pipeline completo de processamento de PDF
- [x] Integra√ß√£o dual LLM (Ollama + OpenAI)
- [x] Sistema de corpus jur√≠dico
- [x] Banco vetorial ChromaDB
- [x] API REST b√°sica
- [x] Sistema de classifica√ß√£o NLP
- [x] An√°lise de documentos legais

### Em Desenvolvimento üîÑ
- [ ] Endpoints de busca e completions
- [ ] Interface web frontend
- [ ] Sistema de cache inteligente
- [ ] M√©tricas de performance
- [ ] Testes automatizados

### Planejado üìã
- [ ] Suporte a m√∫ltiplos formatos (DOCX, TXT)
- [ ] Sistema de usu√°rios e permiss√µes
- [ ] Dashboard de analytics
- [ ] Deployment containerizado
- [ ] CI/CD pipeline

## üéì Conceitos T√©cnicos Avan√ßados

### RAG (Retrieval Augmented Generation)
O sistema implementa RAG atrav√©s da combina√ß√£o de:
- **Retrieval**: ChromaDB para busca vetorial
- **Augmentation**: Context injection nos prompts
- **Generation**: LLM (Ollama/OpenAI) para respostas

### Embeddings e Similaridade
- Vetoriza√ß√£o de documentos usando modelos transformer
- Busca por similaridade cossenoidal
- Indexa√ß√£o HNSW para performance

### Anti-Alucina√ß√£o
- Prompts restritivos: "baseado apenas no documento"
- Par√¢metro `diffusion_of_hallucination`
- Valida√ß√£o de respostas contra fonte

Para inicializar o servidor em modo de desenvolvimento: `python main.py`  
Para desativar hot reload: `python main.py --no-reload`
