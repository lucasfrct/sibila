# Sibila S√≠nica

As sibilas eram figuras da mitologia inspiradas pelos deuses para profetizar.

Esse reposit√≥rio √© um exemplo de aplica√ß√£o capaz de responder perguntas baseadas em documentos fornecidos pelo usu√°rio.

## üöÄ Como Executar Localmente

### Configura√ß√£o R√°pida

1. **Clone o reposit√≥rio**
```bash
git clone https://github.com/lucasfrct/sibila.git
cd sibila
```

2. **Configure o ambiente**
```bash
cp .env.example .env
```

3. **Instale as depend√™ncias**
```bash
pip install -r requirements.txt
```

4. **Instale e configure o Ollama**
```bash
# Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Iniciar o servidor Ollama
ollama serve

# Em outro terminal, baixar o modelo
ollama pull llama3
```

5. **Execute a aplica√ß√£o**
```bash
python main.py
```

A aplica√ß√£o estar√° dispon√≠vel em `http://localhost:3000`

### üìñ Guia Completo
Para instru√ß√µes detalhadas, consulte: **[SETUP_LOCAL.md](./SETUP_LOCAL.md)**

## Estrutura de containers

- `ollama`: A LLM pode ser acessada em <http://localhost:11434>

### Para Executar a aplica√ß√£o

Tudo est√° sendo constru√≠do em docker, e `docker-compose` √© o gerenciador para rodar tudo em desenvolvimento

```docker-compose up -d```

### Ambiente de desenvolvimento

Para rodar a aplica√ß√£o em desenvolvimento, basta rodar o comando abaixo

``` python ./main.py ```

Para rodar a aplica√ß√£o sem hot reload, basta rodar o comando abaixo

``` python ./main.py --no-reload ```

### Estrutura de diret√≥rios

- `./data`: A pasta √© a persist√™ncia dos bancos de dados usados no projeto
- `./dataset`: A pasta com os arquivos de treinamento do modelo e inicializa√ß√£o do banco de dados

### LLM Llama 3

O Meta treinou o Llama 3 com uma contagem de tokens de mais de 15 trilh√µes de tokens.

O modelo 8B tem um limite de conhecimento de mar√ßo de 2023.

O modelo 70B tem um limite de conhecimento de dezembro de 2023.

Os modelos usam Grouped-Query Attention (GQA).

SFT √© uma t√©cnica de treinamento que permite que o modelo seja treinado em um conjunto de dados de treinamento muito maior do que o que pode ser armazenado na mem√≥ria de uma √∫nica GPU.

### Anota√ß√µes

- classificador: <https://medium.com/@andsouit/classificador-de-inten%C3%A7%C3%A3o-com-o-tensorflow-f7fbc854f643>
- LM Studio = interface web para conversar com  o LLM
- Hugging Face: Tom Jobbins
- reposit√≥rio de livros de tecnologia: <https://github.com/free-educa/books/blob/main/books/Design_Patterns.pdf>

## Principais Funcionalidades

A aplica√ß√£o foi organizada em v√°rios m√≥dulos dentro da pasta `src`. A seguir est√£o alguns dos recursos presentes no c√≥digo:

- **Processamento de documentos** (`src/modules/document`): l√™ arquivos PDF, gera metadados de p√°gina, par√°grafo e frase e oferece fun√ß√µes para converter PDF em texto.
- **Gera√ß√£o de corpus** (`src/modules/corpus`): extrai artigos de documentos jur√≠dicos e cria anota√ß√µes autom√°ticas utilizando o m√≥dulo de legisla√ß√£o.
- **An√°lise de legisla√ß√£o** (`src/modules/analysis`): define t√≠tulos de artigos, categorias, entidades, penalidades e cria resumos com apoio de um LLM.
- **Modelos de linguagem** (`src/models`): integra√ß√µes com Ollama ou OpenAI para gerar embeddings e respostas.
- **Banco vetorial** (`src/modules/database/chromadbvector.py`): armazena documentos e permite consultas de similaridade via ChromaDB.
- **Cat√°logo e busca** (`src/modules/catalog`): indexa documentos e monta prompts de busca para o LLM responder com cita√ß√µes.
- **API Flask** (`src/routes`): exp√µe rotas como `/api/v1/corpus`, `/api/v1/dataset` e `/api/v1/health`.

Para inicializar o servidor em modo de desenvolvimento basta executar `python main.py`. Opcionalmente utilize `python main.py --no-reload` para desativar o hot reload.
